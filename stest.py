#!/usr/bin/env python3
'''
significance_analysis_alp.py
──────────────────────────────────────────────────────────────────────────────
**Toy‐Monte-Carlo framework for an ALP (axion-like particle) search in the
signal region**.  The script ingests the *histogram-based pickle* format
generated by the Junda LbyL workflow, isolates the mass window of interest,
and derives―via fast toys―

*   the **median (Asimov) discovery power**  
    \( Z_\text{A} = \sqrt{2\!\left[(s+b)\,\ln\!\bigl(1+s/b\bigr)-s\right]} \)
*   **expected & observed 95 % CL (CL\_s-style) upper limits** on the signal
    yield and corresponding cross-section
*   the **signal strength \(\mu\)** that yields a 5 σ discovery in the data set

It further offers optional overlay plots that contrast the *signal + background*
and *background-only* toy distributions of \(Z\).

-------------------------------------------------------------------------------
Filesystem expectations
-------------------------------------------------------------------------------
The pickle pointed to by ``PKL_PATH`` must have the form

    all_histograms[sample] = {
        "nominal": {
            "h_ZMassZoom":   {"bin_edges": [...], "counts": [...]},
            "h_ZMassFine":   {...},
            ⋯
        },
        "systematics": {
            "EG_SCALE_ALL__1up"     : { <same hist dicts> },
            "EG_SCALE_ALL__1down"   : { … },
            "EG_RESOLUTION__ALL_1up": { … },
            "EG_RESOLUTION__ALL_1down": { … },
        },
        "eff_sr": <float>   # only for ALP & lbyl
    }
    
# One specific ALP mass, verbose ROI tracing
python3 significance_analysis_alp.py --signal alp_20GeV -v

# Loop over *all* ALP masses in the pickle, saving Z-overlay plots
python3 significance_analysis_alp.py --signal all --case overlay

--roi best|full - use only the optimal 80 % signal window (default) or
the full histogram range.
--sigma - scale factor for the Gaussian mass smearing applied
before ROI optimisation.
--ntrials - # toy experiments for each hypothesis (default 10 000).
--seed - fix NumPy's RNG for reproducible toys.
'''

import argparse
import logging
import math
import pickle
import sys
from pathlib import Path

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import gaussian_filter1d

# Constants and default settings
MIN_MASS_EDGE = 5.0  # GeV (minimum mass for ROI)
DEFAULT_BKG = ["lbyl", "yy2ee", "cep"]
REGION = "sr"
PKL_PATH = Path("/home/jtong/lbyl/bkg/bkg_alp_sr_pickle.pkl")
LUMINOSITY = 1.63  # nb^-1, integrated luminosity

# Signal efficiencies (for converting to cross sections)
EFF_MAP = {
    "alp_4GeV": 0.28, "alp_5GeV": 0.30, "alp_6GeV": 0.32, "alp_7GeV": 0.33,
    "alp_8GeV": 0.34, "alp_9GeV": 0.35, "alp_10GeV": 0.36, "alp_12GeV": 0.37,
    "alp_14GeV": 0.38, "alp_15GeV": 0.38, "alp_16GeV": 0.38, "alp_18GeV": 0.39,
    "alp_20GeV": 0.39, "alp_40GeV": 0.40, "alp_60GeV": 0.40, "alp_70GeV": 0.40,
    "alp_80GeV": 0.40, "alp_90GeV": 0.40, "alp_100GeV": 0.40
}

# Nominal cross-sections for signal (nb) indexed by ALP mass [GeV]
alp_sigma_nb = {
    4: 7.96733e3, 5: 6.953744e3, 6: 6.044791e3, 7: 5.30025e3,
    8: 4.67022e3, 9: 4.1546e3, 10: 3.709976e3, 12: 3.016039e3,
    14: 2.499097e3, 15: 2.285133e3, 16: 2.093761e3, 18: 1.782345e3,
    20: 1.526278e3, 30: 7.77903e2, 40: 4.36836e2, 50: 2.600118e2,
    60: 1.604056e2, 70: 1.016849e2, 80: 6.546058e1, 90: 4.280824e1,
    100: 2.824225e1
}

def parse_cli():
    """Parse command-line arguments."""
    p = argparse.ArgumentParser(
        description="Toy MC significance study for ALP search (SR region only)."
    )
    p.add_argument("--signal", default="alp_20GeV",
                   help="Signal key (e.g. 'alp_20GeV') or 'all' for every ALP mass")
    p.add_argument("--bkg", nargs="+", default=DEFAULT_BKG,
                   help=f"Background sample keys (default: {' '.join(DEFAULT_BKG)})")
    p.add_argument("--hist", default="h_ZMassFine",
                   help="Histogram name to analyse (default: h_ZMassFine)")
    p.add_argument("--ntrials", type=int, default=10000,
                   help="Number of toy Monte Carlo experiments")
    p.add_argument("--seed", type=int, default=None,
                   help="Random seed (optional)")
    p.add_argument("--sigma", type=float, default=1.0,
                   help="Gaussian width scale (sigma factor)")
    p.add_argument("--roi", choices=["best", "full"], default="best",
                   help="Region of interest: 'best' (optimal) or 'full'")
    p.add_argument("--case", choices=["overlay", "s_plus_b", "bkg_only"],
                   default="overlay", help="Plotting mode: overlay, s_plus_b, or bkg_only")
    p.add_argument("--use-cls", action="store_true",
                    help="Use CLs-based exclusion instead of Z-based")
    p.add_argument("-v", "--verbose", action="store_true",
                   help="Enable verbose debug logging")
    return p.parse_args()

def load_pickle(path):
    """Load and return pickle data; exit on failure."""
    logging.debug(f"Loading pickle: {path}")
    try:
        with open(path, "rb") as f:
            data = pickle.load(f)
        logging.debug(f"Pickle loaded (keys: {list(data.keys())[:5]}...)")
        return data
    except Exception as e:
        logging.error(f"Cannot read pickle '{path}': {e}")
        sys.exit(f"[FATAL] Cannot read pickle '{path}': {e}")

def cumulative(counts):
    """Cumulative sum with leading zero for fast integrals."""
    return np.concatenate(([0.0], np.cumsum(counts)))

def integral(cum, lo, hi):
    """Integral of counts between indices [lo, hi) using cumulative array."""
    return cum[hi] - cum[lo]

def enlarge_for_bkg(cnt_bkg, edges, lo, hi):
    """
    Expand ROI to include at least 1 background event.
    Moves window outward until background sum >1.
    """
    bkg_cum = cumulative(cnt_bkg)
    if integral(bkg_cum, lo, hi) > 1.0:
        return lo, hi

    left, right = lo, hi
    n_bins = len(cnt_bkg)
    while True:
        can_left = left > 0 and edges[left - 1] >= MIN_MASS_EDGE
        can_right = right < n_bins
        gain_left = cnt_bkg[left - 1] if can_left else -1
        gain_right = cnt_bkg[right] if can_right else -1

        # Expand toward larger gain
        if can_left and gain_left >= gain_right:
            left -= 1
        elif can_right:
            right += 1
        else:
            break

        if integral(cumulative(cnt_bkg), left, right) > 1.0:
            logging.debug(f"Enlarged ROI bins=[{left},{right}) to include background")
            return left, right

    sys.exit("[FATAL] Could not build ROI with >=1 background event.")

def tight_window(cnt_sig, edges, center_idx, frac=0.8):
    """
    Greedily build a symmetric window around center_idx capturing at least frac (was 80% in Run2) of total signal.
    Returns (lo, hi) indices (hi exclusive).
    """
    lo = hi = center_idx
    s_in = cnt_sig[center_idx]
    s_total = cnt_sig.sum()
    if s_total <= 0:
        sys.exit("[FATAL] Empty signal - no ROI possible")
    target = frac * s_total

    logging.debug(f"Start tight window at bin {center_idx}: s_in={s_in}/{s_total}")
    while s_in < target:
        # -------- left candidate --------
        if lo > 0 and edges[lo - 1] >= MIN_MASS_EDGE:
            lo -= 1
            s_in += cnt_sig[lo]
        # -------- right candidate --------
        if hi < len(cnt_sig) - 1:
            hi += 1
            s_in += cnt_sig[hi]
        # If neither side can grow any further we are stuck → break early
        if (lo == 0 or edges[lo - 1] < MIN_MASS_EDGE) and hi == len(cnt_sig) - 1:
            break
        logging.debug(f"Tight window now [{lo},{hi+1}) S_in={s_in}")
    return lo, hi + 1

def find_best_roi(cnt_sig, cnt_bkg, edges):
    """
    Scan all bins as potential ROI centers.
    For each, build an 80%-signal window, enlarge for background >=1,
    and compute expected significance Z = s/sqrt(b).
    Return (lo, hi, best_z) of best ROI.
    """
    centers = 0.5 * (edges[:-1] + edges[1:])
    sig_cum = cumulative(cnt_sig)
    bkg_cum = cumulative(cnt_bkg)

    best_lo = best_hi = -1
    best_z = -np.inf
    for i, m in enumerate(centers):
        if m < MIN_MASS_EDGE:
            continue
        lo, hi = tight_window(cnt_sig, edges, i)
        lo, hi = enlarge_for_bkg(cnt_bkg, edges, lo, hi)
        s = integral(sig_cum, lo, hi)
        b = integral(bkg_cum, lo, hi)
        z = s / math.sqrt(b) if b > 0 else 0.0
        logging.debug(f"ROI center {m:.1f} GeV: bins=[{lo},{hi}), S={s:.2f}, B={b:.2f}, Z={z:.3f}")
        if z > best_z:
            best_lo, best_hi, best_z = lo, hi, z

    if best_lo < 0:
        sys.exit("[FATAL] No valid ROI (80% signal, >=1 bkg) found.")
    logging.info(f"Best ROI bins=[{best_lo},{best_hi}), Z_exp={best_z:.3f}")
    return best_lo, best_hi, best_z

def compute_coarse_binning(sig_counts, edges, min_signal=0.01, merge_above=40.0):
    """
    Merge fine bins into coarser bins above merge_above GeV such that
    each coarse bin has at least min_signal events.
    """
    new_edges = [edges[0]]
    running = 0.0
    for i in range(len(sig_counts)):
        low = edges[i]
        high = edges[i+1]
        running += sig_counts[i]
        
        # Before the threshold: preserve fine binning
        if high <= merge_above:
            new_edges.append(high)
            running = 0.0
        # After threshold: combine until signal exceeds threshold
        elif running >= min_signal:
            new_edges.append(high)
            running = 0.0
    # Always include the final edge
    if new_edges[-1] != edges[-1]:
        new_edges.append(edges[-1])
    return np.array(new_edges)

def compute_asimov_significance(s, b):
    """
    Compute the Asimov (median expected) significance for signal s and background b.
    Formula: Z = sqrt{2[(s+b) ln(1 + s/b) - s]}.
    """
    if s <= 0 or b <= 0:
        return 0.0
    return math.sqrt(2 * ((s + b) * math.log(1 + s / b) - s))

def compute_s95_likelihood(s, b, cl=0.95):
    """
    Simplified CLs-like calculation: find smallest integer n such that
    P(n; b+s) >= cl, returns n (s95).
    """
    from scipy.stats import poisson
    limit = 0
    while poisson.cdf(limit, b + s) < cl:
        limit += 1
    return limit

def get_random_from_hist(bin_edges, bin_contents, rng, n_samples=1):
    """
    Draw random samples from a histogram PDF given by bin_edges and bin_contents.
    Returns array of random x values.
    """
    contents = np.array(bin_contents, dtype=float)
    total = contents.sum()
    if total <= 0:
        return np.zeros(n_samples)
    probs = contents / total
    cdf = np.cumsum(probs)
    r = rng.random(n_samples)
    bins = np.searchsorted(cdf, r, side="right")
    x0 = bin_edges[bins]
    widths = bin_edges[bins+1] - x0
    return x0 + widths * rng.random(n_samples)

def compute_cls95_limit(B_obs, B_exp, S_template, edges, lo, hi, rng, ntrials=10000):
    """
    Compute 95% upper limit on signal using the CLs method via toy MC.
    Returns signal s such that CLs ≈ 0.05.
    """
    # Try a range of signal hypotheses
    s_vals = np.linspace(0, 10 * S_template.sum(), 100)
    for s in s_vals:
        sb_samples = rng.poisson(B_exp + s, size=ntrials)
        b_samples = rng.poisson(B_exp, size=ntrials)
        p_sb = np.mean(sb_samples >= B_obs)
        p_b = np.mean(b_samples >= B_obs)
        if p_b > 0 and (p_sb / p_b) < 0.05:
            return s
    return s_vals[-1]

def plot_overlay(vals_sb, vals_bo, stats_sb, stats_bo, save_path, sig_label, sigma_scale):
    """
    Plot overlay histogram of S+B vs B-only Z distributions with medians and 68% bands.
    """
    med_sb, lo16_sb, hi84_sb = stats_sb
    med_bo, lo16_bo, hi84_bo = stats_bo
    plt.figure(figsize=(8, 5))
    bins = np.histogram_bin_edges(np.concatenate([vals_sb, vals_bo]), bins=100)
    plt.hist(vals_bo, bins=bins, alpha=0.6, color="grey", label="B-only")
    plt.hist(vals_sb, bins=bins, alpha=0.5, color="deepskyblue", label="S+B")
    plt.axvline(med_bo, ls="--", color="grey", label=f"Median B-only = {med_bo:.2f}")
    plt.axvline(med_sb, ls="--", color="deepskyblue", label=f"Median S+B = {med_sb:.2f}")
    plt.axvspan(lo16_bo, hi84_bo, color="grey", alpha=0.2)
    plt.axvspan(lo16_sb, hi84_sb, color="deepskyblue", alpha=0.2)
    plt.xlabel("Z-score")
    plt.ylabel("Toy experiments")
    plt.title(f"Z-distributions for {sig_label} (σ scale={sigma_scale})")
    plt.legend(frameon=False)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()

def plot_distribution(vals, med, lo16, hi84, save_path, mode, sig_label, sigma_scale):
    """
    Plot histogram of Z (or sqrt(B)) values for one case (S+B or B-only) with median and 68% interval.
    """
    finite_vals = vals[np.isfinite(vals)]
    plt.figure(figsize=(7, 5))
    color = "deepskyblue" if mode == "s_plus_b" else "grey"
    plt.hist(finite_vals, bins=150, histtype="stepfilled", alpha=0.75, color=color)
    plt.axvline(med, ls="--", lw=1.2, label=f"median = {med:.2f}")
    plt.axvline(lo16, color="k", ls=":", lw=1)
    plt.axvline(hi84, color="k", ls=":", lw=1, label="68% band")
    xlabel = r"$Z = \sqrt{-2\log\lambda}$" if mode == "s_plus_b" else r"$\sqrt{B}$"
    plt.title(f"{mode.replace('_',' ')} (σ={sigma_scale}) for {sig_label}")
    plt.xlabel(xlabel)
    plt.ylabel("Toy experiments")
    plt.legend()
    plt.tight_layout()
    plt.savefig(save_path, dpi=120)
    plt.close()

def print_limit_summary(sig_key, res):
    """Print a summary of limits and significance for one signal."""
    print("\n===== LIMIT SUMMARY =====")
    print(f"Signal: {sig_key}")
    if 'eff' in res:
        print(f"  • Efficiency: {res['eff']:.3f}")
    if 'sigma_nominal' in res:
        print(f"  • σ_nominal: {res['sigma_nominal']:.4f} nb")
    if 'sigma_exclusion' in res:
        print(f"  • Expected Exclusion: Z={res.get('z_asimov',0):.2f}, σ_95={res['sigma_exclusion']:.4f} nb")
    if 'mu_5sigma' in res:
        print(f"  • Discovery (Z=5): μ_5σ={res['mu_5sigma']:.2f}, σ_5σ={res['sigma_5sigma']:.4f} nb")
        if 'sigma_discovery_lo' in res:
            print(f"    ±1σ: [{res['sigma_discovery_lo']:.3f}, {res['sigma_discovery_hi']:.3f}] nb")
        if 'sigma_discovery_lo2' in res:
            print(f"    ±2σ: [{res['sigma_discovery_lo2']:.3f}, {res['sigma_discovery_hi2']:.3f}] nb")
    if 's95' in res:
        print(f"  • Expected s95 (events): {res['s95']:.2f} [-1σ={res.get('s95_lo',0):.2f}, +1σ={res.get('s95_hi',0):.2f}]")
    if 'sigma_observed' in res:
        print(f"  • Observed σ_95% = {res['sigma_observed']:.4f} nb")

def extract_mass_from_key(sig_key):
    """Extract numeric mass from key string like 'alp_20GeV'."""
    try:
        return float(sig_key.split("_")[1].replace("GeV",""))
    except:
        return -1.0

def run_toys_likelihood(rng, S_roi, B_roi, ntrials, mode="s_plus_b"):
    """
    Generate toy Z-values under two hypotheses:
    - s_plus_b: both signal and background fluctuate (Poisson).
    - bkg_only: only background fluctuates, signal fixed in numerator.
    Returns array of Z = s_toy/sqrt(b_toy).
    """
    zs = []
    for _ in range(ntrials):
        b_toy = rng.poisson(B_roi)
        if b_toy < 1:
            continue
        if mode == "s_plus_b":
            # s_toy = rng.poisson(S_roi)
            s_toy = S_roi
            z = s_toy / math.sqrt(b_toy)
            zs.append(z)
        else:
            s_toy = 0   # bkg only case
            z = math.sqrt(b_toy)
            zs.append(z)
    return np.array(zs)

def process_signal(args, sig_key, pkl):
    """Process one ALP signal sample: ROI, significance, and limits."""
    logging.info(f"Processing signal '{sig_key}'")
    rng = np.random.default_rng(args.seed)

    # Load signal histogram from pickle
    cnt_sig = np.array(pkl[sig_key]["nominal"][args.hist]["counts"], dtype=float)
    edges = np.array(pkl[sig_key]["nominal"][args.hist]["bin_edges"], dtype=float)

    # Sum background histograms
    cnt_bkg = np.zeros_like(cnt_sig)
    for bk in args.bkg:
        cnt_bkg += np.array(pkl[bk]["nominal"][args.hist]["counts"], dtype=float)
    logging.debug(f"Background combined over {len(args.bkg)} samples.")

    # Determine ROI
    if args.roi == "full":
        lo, hi, z_exp = 0, len(cnt_sig), None
    else:
        lo, hi, z_exp = find_best_roi(cnt_sig, cnt_bkg, edges)
    lo_edge, hi_edge = edges[lo], edges[hi]
    logging.info(f"ROI: {lo_edge:.1f}-{hi_edge:.1f} GeV")

    # Rebin to coarse bins if needed
    coarse_edges = compute_coarse_binning(cnt_sig, edges)
    print(f"[{sig_key}] Using coarse binning: {coarse_edges}")
    if len(coarse_edges) > len(edges):
        centers = (edges[:-1]+edges[1:])/2
        cnt_sig, _ = np.histogram(centers, bins=coarse_edges, weights=cnt_sig)
        cnt_bkg, _ = np.histogram(centers, bins=coarse_edges, weights=cnt_bkg)
        edges = coarse_edges
        # Map original fine bin edges to new indices in coarse binning
        lo = np.searchsorted(edges, lo_edge, side="left")
        hi = np.searchsorted(edges, hi_edge, side="right")

    # Skip if no signal
    if cnt_sig.sum() == 0:
        logging.warning(f"{sig_key}: empty signal histogram, skipping.")
        return

    # Fixed S and expected B counts inside ROI
    print(f"Check: signal integral = {cnt_sig.sum():.3f}, metadata = {pkl[sig_key].get('Expected events', 'N/A')}")
    S_roi = cnt_sig[lo:hi].sum()
    B_roi = cnt_bkg[lo:hi].sum()
    logging.debug(
        "[ROI summary] bins=[%d,%d)  S=%.3f  B=%.3f  √B=%.3f",
        lo, hi, S_roi, B_roi, math.sqrt(B_roi)
        )

    if B_roi < 1:
        logging.warning(f"{sig_key}: background in ROI < 1, skipping.")
        return

    # Prepare results entry
    results[sig_key] = {}
    eff = EFF_MAP.get(sig_key, 0.35)  # default to 35% if not found
    results[sig_key]["eff"] = eff

    # Retrieve nominal cross-section
    try:
        mass_val = int(sig_key.split("_")[1].replace("GeV",""))
        sigma_nom = alp_sigma_nb.get(mass_val, None)
        if sigma_nom:
            results[sig_key]["sigma_nominal"] = sigma_nom
    except:
        sigma_nom = None

    # Asimov expected significance and approximate s95
    z_asimov = compute_asimov_significance(S_roi, B_roi)
    results[sig_key]["z_asimov"] = z_asimov
    logging.info("[Asimov Z] %s: s=%.3f, b=%.3f, Z=%.2f", sig_key, S_roi, B_roi, z_asimov)
    asimov_results[sig_key] = (S_roi, B_roi, z_asimov)
    s95_approx = 1.64 * math.sqrt(B_roi)  # Gaussian approximation
    results[sig_key]["s95"] = s95_approx

    # Toy-based expected s95 (simulate background-only toys)
    toy_s95 = []
    for _ in range(args.ntrials):
        b_toy = rng.poisson(B_roi)
        if b_toy < 1:
            continue
        s95_toy = compute_s95_likelihood(0, b_toy)
        toy_s95.append(s95_toy)
    toy_s95 = np.array(toy_s95)
    results[sig_key]["s95_lo"] = np.percentile(toy_s95, 16)
    results[sig_key]["s95_hi"] = np.percentile(toy_s95, 84)
    results[sig_key]["sigma95"] = s95_approx / (eff * LUMINOSITY) if eff > 0 else float('nan')

    # Observed limit (one toy experiment)
    B_obs = rng.poisson(B_roi)
    if B_obs < 1:
        B_obs = 1
    if args.use_cls:
        s95_obs = compute_cls95_limit(B_obs, B_roi, cnt_sig, edges, lo, hi, rng, ntrials=100)
    else:
        s95_obs = compute_s95_likelihood(B_obs, B_roi)
    results[sig_key]["sigma_observed"] = s95_obs / (eff * LUMINOSITY) if eff > 0 else float('nan')

    # Discovery threshold: find mu for which Z_obs=5 using Asimov formula
    def find_mu_for_Z(mu_guess=1.0, target=5.0, tol=1e-3, max_iter=50):
        mu = mu_guess
        for _ in range(max_iter):
            Z = compute_asimov_significance(mu * S_roi, B_obs)
            if abs(Z - target) < tol:
                return mu
            mu *= target / max(Z, 1e-6)  # adjust by ratio, avoid divide-by-zero
        return mu

    mu_5 = find_mu_for_Z()
    results[sig_key]["mu_5sigma"] = mu_5
    results[sig_key]["sigma_5sigma"] = mu_5 / (eff * LUMINOSITY) if eff > 0 else float('nan')
    mu_5sigma = results[sig_key]["sigma_5sigma"]
    logging.info(f"[Discovery] μ (Z_obs = 5) = {mu_5:.2f}")
    logging.info(f"[Discovery Z=5] {sig_key}: σ = {mu_5sigma:.3f} nb")
    
    # Discovery (Z=2) distribution using toy formula mu = 5*sqrt(B)/S
    mu_trials = []
    for _ in range(args.ntrials):
        b_toy = rng.poisson(B_roi)
        if S_roi == 0 or b_toy == 0:
            continue
        mu_trials.append(5.0 * math.sqrt(b_toy) / S_roi)
    mu_trials = np.array(mu_trials)
    results[sig_key]["mu_discovery_lo1"] = np.percentile(mu_trials, 16)
    results[sig_key]["mu_discovery_hi1"] = np.percentile(mu_trials, 84)
    results[sig_key]["mu_discovery_lo2"] = np.percentile(mu_trials, 2.5)
    results[sig_key]["mu_discovery_hi2"] = np.percentile(mu_trials, 97.5)
    results[sig_key]["sigma_discovery_lo"] = results[sig_key]["mu_discovery_lo1"] / (eff * LUMINOSITY) if eff > 0 else float('nan')
    results[sig_key]["sigma_discovery_hi"] = results[sig_key]["mu_discovery_hi1"] / (eff * LUMINOSITY) if eff > 0 else float('nan')
    results[sig_key]["sigma_discovery_lo2"] = results[sig_key]["mu_discovery_lo2"] / (eff * LUMINOSITY) if eff > 0 else float('nan')
    results[sig_key]["sigma_discovery_hi2"] = results[sig_key]["mu_discovery_hi2"] / (eff * LUMINOSITY) if eff > 0 else float('nan')

    # Overlay plot if requested
    if args.case == "overlay":
        vals_sb = run_toys_likelihood(rng, S_roi, B_roi, args.ntrials)
        vals_bo = run_toys_likelihood(rng, S_roi, B_roi, args.ntrials, mode="bkg_only")
        stats_sb = (np.median(vals_sb), *np.percentile(vals_sb, [16, 84]))
        stats_bo = (np.median(vals_bo), *np.percentile(vals_bo, [16, 84]))
        out_dir = Path("Significance_dis") / REGION / sig_key
        out_dir.mkdir(parents=True, exist_ok=True)
        plot_overlay(vals_sb, vals_bo, stats_sb, stats_bo, out_dir / "overlay.png", sig_key, args.sigma)

def main():
    args = parse_cli()
    level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(level=level, format="%(asctime)s [%(levelname)s] %(message)s")

    if not PKL_PATH.is_file():
        sys.exit(f"[FATAL] Pickle '{PKL_PATH}' not found.")
    pkl = load_pickle(PKL_PATH)

    # Determine signals to process
    if args.signal.lower() == "all":
        signals = sorted(k for k in pkl.keys() if k.startswith("alp_"))
    else:
        signals = [args.signal]

    # Process each signal
    for sig in signals:
        process_signal(args, sig, pkl)

    # Plot expected Asimov Z vs mass if available
    if asimov_results:
        masses = []
        zvals = []
        for key, (_, _, z) in asimov_results.items():
            try:
                m = float(key.split("_")[1].replace("GeV",""))
                masses.append(m); zvals.append(z)
            except:
                continue
        masses, zvals = zip(*sorted(zip(masses, zvals)))
        plt.figure()
        plt.plot(masses, zvals, 'o-', color='blue')
        plt.xlabel("ALP Mass [GeV]")
        plt.ylabel("Expected Z (Asimov)")
        plt.grid(True)
        plt.tight_layout()
        plt.savefig("expected_Z_vs_mass.png")
        plt.close()

    # Print summary for each signal
    print("\n===== ALL LIMIT SUMMARIES =====")
    for sig in sorted(results.keys(), key=extract_mass_from_key):
        print_limit_summary(sig, results[sig])

if __name__ == "__main__":
    asimov_results = {}
    results = {}
    main()
